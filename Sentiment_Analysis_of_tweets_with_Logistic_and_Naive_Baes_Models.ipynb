{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk0YhmCo2B5T"
      },
      "source": [
        "**This notebook performs the following tasks on the tweets dataset**\n",
        "* Extract features from a text using tokenization, stemming + remove useless words and characters\n",
        "* Implement logistic regression from scratch\n",
        "  * Apply logistic regression on a natural language processing task\n",
        "  * Test using your logistic regression\n",
        "* Implement Naive Bayes from scratch\n",
        "  * Apply Naive Bayes on a natural language processing task\n",
        "  * Test using your Naive Bayes model\n",
        "* Perform error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk2GxvZPKJVO"
      },
      "outputs": [],
      "source": [
        "# Load required modules\n",
        "import nltk   \n",
        "from os import getcwd                             # Python library for NLP\n",
        "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import random                              # pseudo-random number generator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_q7E4NmKSet"
      },
      "outputs": [],
      "source": [
        "# downloads sample twitter dataset.\n",
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EAIg482KZ7h"
      },
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKLKWD9EKy9q"
      },
      "outputs": [],
      "source": [
        "print('Number of positive tweets: ', len(all_positive_tweets))\n",
        "print('Number of negative tweets: ', len(all_negative_tweets))\n",
        "\n",
        "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
        "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5fQbjKnK3B2"
      },
      "outputs": [],
      "source": [
        "# Declare a figure with a custom size\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "\n",
        "# labels for the two classes\n",
        "labels = 'Positives', 'Negative'\n",
        "\n",
        "# Sizes for each slide\n",
        "sizes = [len(all_positive_tweets), len(all_negative_tweets)] \n",
        "\n",
        "# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "\n",
        "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.axis('equal')  \n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWwgVcJTLGQY"
      },
      "outputs": [],
      "source": [
        "# print positive in greeen\n",
        "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
        "\n",
        "# print negative in red\n",
        "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V1RQFP9LiO6"
      },
      "source": [
        "## Preprocess raw text for Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2-dmTwmLnbq"
      },
      "source": [
        "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
        "\n",
        "* Tokenizing the string\n",
        "* Lowercasing\n",
        "* Removing stop words and punctuation\n",
        "* Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5E0IlFJLZ7a"
      },
      "outputs": [],
      "source": [
        "# Our selected sample. Complex enough to exemplify each step\n",
        "tweet = all_positive_tweets[2277]\n",
        "print(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go7FMelvLqUf"
      },
      "outputs": [],
      "source": [
        "# download the stopwords from NLTK\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIZY7HbaLskT"
      },
      "outputs": [],
      "source": [
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAZl3I-PLycs"
      },
      "outputs": [],
      "source": [
        "print('\\033[92m' + tweet)\n",
        "print('\\033[94m')\n",
        "\n",
        "# remove old style retweet text \"RT\"\n",
        "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "print(tweet2)\n",
        "# remove hyperlinks\n",
        "tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
        "print(tweet2)\n",
        "# remove hashtags\n",
        "# only removing the hash # sign from the word\n",
        "tweet2 = re.sub(r'#', '', tweet2)\n",
        "\n",
        "print(tweet2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiRXq45fL4Oe"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print('\\033[92m' + tweet2)\n",
        "print('\\033[94m')\n",
        "\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "tweet_tokens = tokenizer.tokenize(tweet2)\n",
        "\n",
        "print()\n",
        "print('Tokenized string:')\n",
        "print(tweet_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARBjiMAMNZ2B"
      },
      "source": [
        "### Tokenize the string\n",
        "\n",
        "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The [tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual) module from NLTK allows us to do these easily:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-iOiAYAOGQl"
      },
      "source": [
        "### Remove stop words and punctuations\n",
        "\n",
        "The next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. You'll see the list provided by NLTK when you run the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVEMQVdHNtMQ"
      },
      "outputs": [],
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english') \n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ShNzixEOafG"
      },
      "source": [
        "We can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\n",
        "\n",
        "For the punctuation, we saw earlier that certain groupings like ':)' and '...' should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\n",
        "\n",
        "Time to clean up our tokenized tweet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEob0xz9ORYP"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print('\\033[92m')\n",
        "print(tweet_tokens)\n",
        "print('\\033[94m')\n",
        "\n",
        "tweets_clean = []\n",
        "\n",
        "for word in tweet_tokens: # Go through every word in your tokens list\n",
        "    if (word not in stopwords_english and  # remove stopwords\n",
        "        word not in string.punctuation):  # remove punctuation\n",
        "        tweets_clean.append(word)\n",
        "\n",
        "print('removed stop words and punctuation:')\n",
        "print(tweets_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxF488j3Oq_f"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n",
        "\n",
        "Consider the words: \n",
        " * **learn**\n",
        " * **learn**ing\n",
        " * **learn**ed\n",
        " * **learn**t\n",
        " \n",
        "All these words are stemmed from its common root **learn**. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, **happi** and **sunni**. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n",
        "\n",
        " * **happ**y\n",
        " * **happi**ness\n",
        " * **happi**er\n",
        " \n",
        "We can see that the prefix **happi** is more commonly used. We cannot choose **happ** because it is the stem of unrelated words like **happen**.\n",
        " \n",
        "NLTK has different modules for stemming and we will be using the [PorterStemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) module which uses the [Porter Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/). Let's see how we can use it in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ijvldpdOeMw"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print('\\033[92m')\n",
        "print(tweets_clean)\n",
        "print('\\033[94m')\n",
        "\n",
        "# Instantiate stemming class\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "# Create an empty list to store the stems\n",
        "tweets_stem = [] \n",
        "\n",
        "for word in tweets_clean:\n",
        "    stem_word = stemmer.stem(word)  # stemming word\n",
        "    tweets_stem.append(stem_word)  # append to the list\n",
        "\n",
        "print('stemmed words:')\n",
        "print(tweets_stem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiyLYLPnOtgq"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks    \n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzTOPGFaO9-h"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            freqs[pair] = freqs.get(pair, 0) + 1\n",
        "    return freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qrZIXXoPA3P"
      },
      "outputs": [],
      "source": [
        "# choose the same tweet\n",
        "tweet = all_positive_tweets[2277]\n",
        "\n",
        "print()\n",
        "print('\\033[92m')\n",
        "print(tweet)\n",
        "print('\\033[94m')\n",
        "\n",
        "# call the imported function\n",
        "tweets_stem = process_tweet(tweet); # Preprocess a given tweet\n",
        "\n",
        "print('preprocessed tweet:')\n",
        "print(tweets_stem) # Print the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iayMThTeT6q6"
      },
      "source": [
        "Next, we will build an array of labels that matches the sentiments of our tweets.  This data type works pretty much like a regular list but is optimized for computations and manipulation. The `labels` array will be composed of 10000 elements. The first 5000 will be filled with `1` labels denoting positive sentiments, and the next 5000 will be `0` labels denoting the opposite. We can do this easily with a series of operations provided by the `numpy` library:\n",
        "\n",
        "* `np.ones()` - create an array of 1's\n",
        "* `np.zeros()` - create an array of 0's\n",
        "* `np.append()` - concatenate arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOXuzw3yPNhw"
      },
      "outputs": [],
      "source": [
        "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
        "tweets = all_positive_tweets + all_negative_tweets\n",
        "# make a numpy array representing labels of the tweets\n",
        "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4awr0_-T0Ic"
      },
      "outputs": [],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(tweets, labels)\n",
        "\n",
        "# check data type\n",
        "print(f'type(freqs) = {type(freqs)}')\n",
        "\n",
        "# check length of the dictionary\n",
        "print(f'len(freqs) = {len(freqs)}')\n",
        "print(freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1wzBWBpU1OO"
      },
      "outputs": [],
      "source": [
        "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
        "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
        "        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n",
        "        'song', 'idea', 'power', 'play', 'magnific']\n",
        "\n",
        "# list representing our table of word counts.\n",
        "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
        "data = []\n",
        "\n",
        "# loop through our selected words\n",
        "for word in keys:\n",
        "    \n",
        "    # initialize positive and negative counts\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    \n",
        "    # retrieve number of positive counts\n",
        "    if (word, 1) in freqs:\n",
        "        pos = freqs[(word, 1)]\n",
        "        \n",
        "    # retrieve number of negative counts\n",
        "    if (word, 0) in freqs:\n",
        "        neg = freqs[(word, 0)]\n",
        "        \n",
        "    # append the word counts to the table\n",
        "    data.append([word, pos, neg])\n",
        "    \n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUsG_Sh2VOKf"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
        "x = np.log([x[1] + 1 for x in data])  \n",
        "\n",
        "# do the same for the negative counts\n",
        "y = np.log([x[2] + 1 for x in data]) \n",
        "\n",
        "# Plot a dot for each pair of words\n",
        "ax.scatter(x, y)  \n",
        "\n",
        "# assign axis labels\n",
        "plt.xlabel(\"Log Positive count\")\n",
        "plt.ylabel(\"Log Negative count\")\n",
        "\n",
        "# Add the word as the label at the same position as you added the points just before\n",
        "for i in range(0, len(data)):\n",
        "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
        "\n",
        "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RCCtnXii4kI"
      },
      "source": [
        "#Logistic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RcAbjthkiEKR"
      },
      "outputs": [],
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZhFWQq2ZiMi1"
      },
      "outputs": [],
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ij4RQYP3iiIl"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NrMAcgrhikLk",
        "outputId": "84733300-a71e-4a0f-8d66-8d082284f28c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11428\n"
          ]
        }
      ],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uXXQ58Qii2CT",
        "outputId": "653fc33e-1173-4e4d-f3ea-f6d8a31f10de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ],
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zepXvXqpi9vJ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1/(1+np.exp(-z))\n",
        "    return h\n",
        "  \n",
        "# gradientDescent\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = len(x)\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = -1/m * np.sum(y*np.log(h)+(1-y)*np.log(1-h))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta-alpha*np.dot(x.T,(h-y))/m\n",
        "        \n",
        "    J = float(J)\n",
        "    return J, theta\n",
        "\n",
        "# extract_features\n",
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "        \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word,1),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word,0),0)\n",
        "        \n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPJ5mYOQmPlm",
        "outputId": "9df56401-52db-434d-ff62-bc38c255a7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cost after training is 0.22521260.\n",
            "The resulting vector of weights is [6e-08, 0.0005382, -0.0005583]\n"
          ]
        }
      ],
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIzW_5rhm2l-"
      },
      "outputs": [],
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogVyDPqTnlno",
        "outputId": "4a66ab1c-5105-4ae0-d375-062f3cf992dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am happy -> 0.519275\n",
            "I am bad -> 0.494347\n",
            "this movie should have been great. -> 0.515980\n",
            "great -> 0.516065\n",
            "great great -> 0.532097\n",
            "great great great -> 0.548063\n",
            "great great great great -> 0.563930\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gTCx9HAnn8Q",
        "outputId": "6dd525f9-aad9-4977-e9ec-5e1641387b7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.83110766]])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXpv190VnuNA"
      },
      "outputs": [],
      "source": [
        "# test_logistic_regression\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    m=len(test_y)\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    preds=np.array(y_hat)>0.5\n",
        "    preds=preds.reshape(-1,1)\n",
        "    accuracy = np.sum(preds==test_y.reshape(-1,1))/m    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dxq8-OKrHsi",
        "outputId": "0e079beb-d0a1-431e-867a-0124eb2fc371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "1\t0.48942982\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
            "http://t.co/UGQzOx0huu\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418982\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418982\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418982\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "1\t0.49636406\tb'park get sunlight'\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "1\t0.48250522\tb'uff itna miss karhi thi ap :p'\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50988296\tb'u prob fun david'\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50040366\tb'pat jay'\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000002\tb'belov grandmoth'\n",
            "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
            "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
            "0\t0.50648699\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
          ]
        }
      ],
      "source": [
        "# Some error analysis done for you\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxLolmLErd3B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSsTeoqI1-o"
      },
      "source": [
        "# Naive Baes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFjgKSDTPHGi"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyAHeQWHPBXF"
      },
      "outputs": [],
      "source": [
        "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
        "    \"\"\"\n",
        "    Create a plot of the covariance confidence ellipse of `x` and `y`\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : array_like, shape (n, )\n",
        "        Input data.\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axes object to draw the ellipse into.\n",
        "    n_std : float\n",
        "        The number of standard deviations to determine the ellipse's radiuses.\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.patches.Ellipse\n",
        "    Other parameters\n",
        "    ----------------\n",
        "    kwargs : `~matplotlib.patches.Patch` properties\n",
        "    \"\"\"\n",
        "    if x.size != y.size:\n",
        "        raise ValueError(\"x and y must be the same size\")\n",
        "\n",
        "    cov = np.cov(x, y)\n",
        "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    # Using a special case to obtain the eigenvalues of this\n",
        "    # two-dimensionl dataset.\n",
        "    ell_radius_x = np.sqrt(1 + pearson)\n",
        "    ell_radius_y = np.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0),\n",
        "                      width=ell_radius_x * 2,\n",
        "                      height=ell_radius_y * 2,\n",
        "                      facecolor=facecolor,\n",
        "                      **kwargs)\n",
        "\n",
        "    # Calculating the stdandard deviation of x from\n",
        "    # the squareroot of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
        "    mean_x = np.mean(x)\n",
        "\n",
        "    # calculating the stdandard deviation of y ...\n",
        "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    transf = transforms.Affine2D() \\\n",
        "        .rotate_deg(45) \\\n",
        "        .scale(scale_x, scale_y) \\\n",
        "        .translate(mean_x, mean_y)\n",
        "\n",
        "    ellipse.set_transform(transf + ax.transData)\n",
        "    return ax.add_patch(ellipse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT-xfBl7Pbmf",
        "outputId": "bdd63a3a-b820-4cb2-f258-00012a16e04f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)\n",
        "\n",
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPvxisO8QApw"
      },
      "source": [
        "# Part 1: Process the Data\n",
        "\n",
        "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
        "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
        "- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n",
        "- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
        "- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
        "\n",
        "We have given you the function `process_tweet` that does this for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOrUCKx_P-vv",
        "outputId": "fae63d60-9d23-463b-c41d-9de34ce90195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
          ]
        }
      ],
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "# print cleaned tweet\n",
        "print(process_tweet(custom_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWzU6huLQjU_"
      },
      "source": [
        "## Part 1.1 Implementing your helper functions\n",
        "\n",
        "To help you train your naive bayes model, you will need to compute a dictionary where the keys are a tuple (word, label) and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
        "\n",
        "You will also implement a lookup helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
        "\n",
        "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
        "\n",
        "{\n",
        "    (\"rather\", 1): 2,\n",
        "    (\"happi\", 1) : 1, \n",
        "    (\"excit\", 1) : 1\n",
        "}\n",
        "\n",
        "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
        "- Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n",
        "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
        "\n",
        "#### Instructions\n",
        "Create a function `count_tweets` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
        "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
        "- The value the number of times this word appears in the given collection of tweets (an integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLQRZWqpQHg6"
      },
      "outputs": [],
      "source": [
        "# count_tweets function\n",
        "\n",
        "def count_tweets(result, tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        result: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            # define the key, which is the word and label tuple\n",
        "            pair = (word,y)\n",
        "            \n",
        "            # if the key exists in the dictionary, increment the count\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "\n",
        "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAizDBX9Qn2q",
        "outputId": "79da1ed4-d462-4b75-f4a5-21b02f870a83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the function\n",
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxEBC1SzRDte"
      },
      "source": [
        "# Part 2: Train your model using Naive Bayes\n",
        "\n",
        "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
        "\n",
        "#### So how do you train a Naive Bayes classifier?\n",
        "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
        "- You will create a probability for each class.\n",
        "$P(D_{pos})$ is the probability that the document is positive.\n",
        "$P(D_{neg})$ is the probability that the document is negative.\n",
        "Use the formulas as follows and store the values in a dictionary:\n",
        "\n",
        "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
        "\n",
        "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
        "\n",
        "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G34BKC2uRjdr"
      },
      "source": [
        "#### Prior and Logprior\n",
        "\n",
        "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
        "\n",
        "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
        "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
        "\n",
        "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
        "\n",
        "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
        "\n",
        "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTFrB75WRDxA"
      },
      "source": [
        "#### Positive and Negative Probability of a Word\n",
        "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
        "\n",
        "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
        "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
        "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
        "\n",
        "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrD-o8KTRLv8"
      },
      "source": [
        "#### Log likelihood\n",
        "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
        "\n",
        "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbhkmNeCRdST"
      },
      "source": [
        "##### Create `freqs` dictionary\n",
        "- Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
        "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
        "- The value is the number of times it has appeared.\n",
        "\n",
        "We will use this dictionary in several parts of this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M17gY-4tQ8jO"
      },
      "outputs": [],
      "source": [
        "# Build the freqs dictionary for later uses\n",
        "freqs = count_tweets({}, train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS8e8NjgR0a6"
      },
      "source": [
        "#### Instructions\n",
        "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
        "\n",
        "##### Calculate $V$\n",
        "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
        "\n",
        "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
        "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
        "\n",
        "##### Calculate $N_{pos}$, and $N_{neg}$\n",
        "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
        "\n",
        "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
        "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
        "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
        "\n",
        "##### Calculate the logprior\n",
        "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
        "\n",
        "##### Calculate log likelihood\n",
        "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
        "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
        "\n",
        "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "257WJ64pRwdO"
      },
      "outputs": [],
      "source": [
        "# train_naive_bayes\n",
        "\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = set([word[0] for word in freqs.keys()])\n",
        "    V = len(vocab)    \n",
        "\n",
        "    # calculate N_pos, N_neg, V_pos, V_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += freqs[pair]\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += freqs[pair]\n",
        "    \n",
        "    # Calculate D, the number of documents\n",
        "    D = len(train_y)\n",
        "\n",
        "    # Calculate D_pos, the number of positive documents\n",
        "    D_pos = sum(train_y)\n",
        "\n",
        "    # Calculate D_neg, the number of negative documents\n",
        "    D_neg = D-D_pos\n",
        "\n",
        "    # Calculate logprior\n",
        "    logprior = np.log(D_pos/D_neg)\n",
        "    \n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = freqs.get((word,1),0)\n",
        "        freq_neg = freqs.get((word,0),0)\n",
        "\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = (freq_pos+1)/(N_pos+V)\n",
        "        p_w_neg = (freq_neg+1)/(N_neg+V)\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "\n",
        "    return logprior, loglikelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7nOh_HOUa3e",
        "outputId": "2272c781-e358-4694-b313-c2daa4dcd068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n",
            "9162\n"
          ]
        }
      ],
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pYbuDIAUirV"
      },
      "source": [
        "# Part 3: Test your naive bayes\n",
        "\n",
        "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n",
        "\n",
        "#### Implement `naive_bayes_predict`\n",
        "**Instructions**:\n",
        "Implement the `naive_bayes_predict` function to make predictions on tweets.\n",
        "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
        "* It returns the probability that the tweet belongs to the positive or negative class.\n",
        "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
        "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
        "\n",
        "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
        "\n",
        "#### Note\n",
        "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n",
        "\n",
        "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3IV2JfVUd_C"
      },
      "outputs": [],
      "source": [
        "# naive_bayes_predict\n",
        "\n",
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    # process the tweet to get a list of words\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # initialize probability to zero\n",
        "    p = 0\n",
        "\n",
        "    # add the logprior\n",
        "    p += logprior\n",
        "\n",
        "    for word in word_l:\n",
        "\n",
        "        # check if the word exists in the loglikelihood dictionary\n",
        "        if word in loglikelihood:\n",
        "            # add the log likelihood of that word to the probability\n",
        "            p += loglikelihood[word]\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5ChuONyVAQr",
        "outputId": "d8d58add-11a6-404e-c132-34b5f74d3a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The expected output is 1.5574658811595445\n"
          ]
        }
      ],
      "source": [
        "# Experiment with your own tweet.\n",
        "my_tweet = 'She smiled.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Jym6BDVUTV"
      },
      "source": [
        "#### Implement test_naive_bayes\n",
        "**Instructions**:\n",
        "* Implement `test_naive_bayes` to check the accuracy of your predictions.\n",
        "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
        "* It returns the accuracy of your model.\n",
        "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYWEam-vVDDu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  # return this properly\n",
        "\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    error = np.mean(np.abs(np.array(test_y)-np.array(y_hats)))\n",
        "\n",
        "    # Accuracy is 1 minus the error\n",
        "    accuracy = 1-error\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG0wktvtWMJJ",
        "outputId": "6a79693a-53e5-4cc1-a779-60102620d2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy = 0.9955\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8_O7mikWNoO",
        "outputId": "2e9916ed-5adc-4dcc-fb87-eb98c4ca582d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am happy -> 2.14\n",
            "I am bad -> -1.31\n",
            "this movie should have been great. -> 2.12\n",
            "great -> 2.13\n",
            "great great -> 4.26\n",
            "great great great -> 6.39\n",
            "great great great great -> 8.52\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to the your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:    \n",
        "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "    print(f'{tweet} -> {p:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TFRTZTYWRWn",
        "outputId": "311db452-7d5d-4fa1-9f2b-2a161a76a5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am happy that I did not go -> 1.74\n",
            "I am not happy that I went -> 1.19\n"
          ]
        }
      ],
      "source": [
        "# Naive bayes is not able to differntiate meaning when we change the order, and use negatives\n",
        "for tweet in ['I am happy that I did not go','I am not happy that I went']:    \n",
        "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "    print(f'{tweet} -> {p:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIcEBRgXHP2"
      },
      "source": [
        "# Part 4: Filter words by Ratio of positive to negative counts\n",
        "\n",
        "- Some words have more positive counts than others, and can be considered \"more positive\".  Likewise, some words can be considered more negative than others.\n",
        "- One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n",
        "    - Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.\n",
        "- We can calculate the ratio of positive to negative frequencies of a word.\n",
        "- Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n",
        "- Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).\n",
        "\n",
        "#### Implement get_ratio\n",
        "- Given the freqs dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.\n",
        "- Similarly, use the `lookup` function to get the negative count of that word.\n",
        "- Calculate the ratio of positive divided by negative counts\n",
        "\n",
        "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n",
        "\n",
        "Where pos_words and neg_words correspond to the frequency of the words in their respective classes. \n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>Words</b>\n",
        "        </td>\n",
        "        <td>\n",
        "        Positive word count\n",
        "        </td>\n",
        "         <td>\n",
        "        Negative Word Count\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        glad\n",
        "        </td>\n",
        "         <td>\n",
        "        41\n",
        "        </td>\n",
        "    <td>\n",
        "        2\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        arriv\n",
        "        </td>\n",
        "         <td>\n",
        "        57\n",
        "        </td>\n",
        "    <td>\n",
        "        4\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        :(\n",
        "        </td>\n",
        "         <td>\n",
        "        1\n",
        "        </td>\n",
        "    <td>\n",
        "        3663\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        :-(\n",
        "        </td>\n",
        "         <td>\n",
        "        0\n",
        "        </td>\n",
        "    <td>\n",
        "        378\n",
        "        </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz31Za5_Wkrj"
      },
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Output:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  # freqs.get((word, label), 0)\n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCEfkpmXifS",
        "outputId": "d804444f-f00c-4711-c21e-f7618c902190"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'positive': 162, 'negative': 18, 'ratio': 8.578947368421053}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def get_ratio(freqs, word):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary containing the words\n",
        "\n",
        "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
        "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
        "    '''\n",
        "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
        "\n",
        "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
        "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
        "    \n",
        "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
        "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
        "    \n",
        "    # calculate the ratio of positive to negative counts for the word\n",
        "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive']+1) / (pos_neg_ratio['negative']+1)\n",
        "\n",
        "    return pos_neg_ratio\n",
        "get_ratio(freqs, 'happi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM_xautkX5Il"
      },
      "source": [
        "#### Implement get_words_by_threshold(freqs,label,threshold)\n",
        "\n",
        "* If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n",
        "* If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n",
        "* Use the `get_ratio` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n",
        "* Append the `get_ratio` dictionary inside another dictinoary, where the key is the word, and the value is the dictionary `pos_neg_ratio` that is returned by the `get_ratio` function.\n",
        "An example key-value pair would have this structure:\n",
        "```\n",
        "{'happi':\n",
        "    {'positive': 10, 'negative': 20, 'ratio': 0.524}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbfSzyH1X1hb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary of words\n",
        "        label: 1 for positive, 0 for negative\n",
        "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
        "    Output:\n",
        "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
        "        example of a key value pair:\n",
        "        {'happi':\n",
        "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
        "        }\n",
        "    '''\n",
        "    word_list = {}\n",
        "\n",
        "    for key in freqs.keys():\n",
        "        word, _ = key\n",
        "\n",
        "        # get the positive/negative ratio for a word\n",
        "        pos_neg_ratio = get_ratio(freqs, word)\n",
        "\n",
        "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
        "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
        "        \n",
        "            # Add the pos_neg_ratio to the dictionary\n",
        "            word_list[word] = pos_neg_ratio\n",
        "\n",
        "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
        "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
        "        \n",
        "            # Add the pos_neg_ratio to the dictionary\n",
        "            word_list[word] = pos_neg_ratio\n",
        "\n",
        "        # otherwise, do not include this word in the list (do nothing)\n",
        "\n",
        "    return word_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCwYJbSOaPYo",
        "outputId": "798b85ce-ccf3-4df3-eab1-89814cf3eec5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{':(': {'positive': 1, 'negative': 3675, 'ratio': 0.000544069640914037},\n",
              " ':-(': {'positive': 0, 'negative': 386, 'ratio': 0.002583979328165375},\n",
              " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
              " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
              " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
              " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
              " '‚ôõ': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
              " '„Äã': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
              " 'beliÃáev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'wiÃáll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'justiÃán': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'ÔΩìÔΩÖÔΩÖ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'ÔΩçÔΩÖ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test your function: find negative words at or below a threshold\n",
        "get_words_by_threshold(freqs, label=0, threshold=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up-8qCHraSLD",
        "outputId": "3fcc919e-3311-4fdb-e3b4-949cbdfd3f1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
              " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
              " ':)': {'positive': 2960, 'negative': 2, 'ratio': 987.0},\n",
              " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
              " ':d': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
              " ':p': {'positive': 105, 'negative': 0, 'ratio': 106.0},\n",
              " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
              " ':-)': {'positive': 552, 'negative': 0, 'ratio': 553.0},\n",
              " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
              " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
              " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
              " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
              " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
              " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
              " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
              " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
              " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
              " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
              " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
              " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
              " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
              " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
              " '‚Üê': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
              " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test your function; find positive words at or above a threshold\n",
        "get_words_by_threshold(freqs, label=1, threshold=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wPPk4zGaXxG"
      },
      "source": [
        "# Part 5: Error Analysis\n",
        "\n",
        "In this part you will see some tweets that your model missclassified. Why do you think the missclassifications happened? Were there any assumptions made by your naive bayes model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ9Zeu9_aT2i",
        "outputId": "2b789801-a922-4b67-cde7-d142e9bce3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Truth Predicted Tweet\n",
            "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
            "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
            "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :d'\n",
            "1\t0.00\tb'park get sunlight'\n",
            "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
            "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
            "0\t1.00\tb'u prob fun david'\n",
            "0\t1.00\tb'pat jay'\n",
            "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
          ]
        }
      ],
      "source": [
        "# Some error analysis done for you\n",
        "print('Truth Predicted Tweet')\n",
        "for x, y in zip(test_x, test_y):\n",
        "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
        "    if y != (np.sign(y_hat) > 0):\n",
        "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
        "            process_tweet(x)).encode('ascii', 'ignore')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kor7HPUPaZYU",
        "outputId": "457b46da-6e6f-43d2-c677-6bf61b7d7f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.1925208659734725\n"
          ]
        }
      ],
      "source": [
        "# Test with your own tweet - feel free to modify `my_tweet`\n",
        "my_tweet = 'I am happy because you are checking out this code <3'\n",
        "\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print(p)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EWzU6huLQjU_",
        "G34BKC2uRjdr",
        "RTFrB75WRDxA",
        "yrD-o8KTRLv8",
        "pS8e8NjgR0a6",
        "v-Jym6BDVUTV",
        "kM_xautkX5Il"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}